{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Provenance integration in netcdf/xarray Data-Intensive workflows\n",
    "\n",
    "\n",
    "#### Authors: Alessandro Spinuso and Andrej Mihajlovski\n",
    "\n",
    " \n",
    "####  Royal Netherlands Meteorological Institure (KNMI)\n",
    "\n",
    "\n",
    "The following \"Live\" notebook demonstrates a simple workflow implemented with a data-intensive processing library (dispel4py),  that has been extended with a configurable and programmable provenance tracking framework (Active provenance).\n",
    "\n",
    "## Highligts - Active Provenance, S-PROV and S-ProvFlow\n",
    "<ul>\n",
    "<li>\n",
    "The provenance information produced can be tuned and adapted to computational, precision and contextualisation requirements</li>\n",
    "<li>\n",
    "The Active freamework allows for the traceability of data-reuse across different executions, methods and users</li>\n",
    "<li>The provenance can be stored as files or sent at run-time to an external repository (S-ProvFlow)</li>\n",
    "<li>The repository can be searched and explored via interactive tools</li> \n",
    "<li>The provenance model is designed around an hybrid data-flow model, which takes into account data-streams and concrente data resourcese. eg. file location, webservices etc.</li>\n",
    "<li>The lineage can be exported from the repository in W3C PROV format. This facilitates the production of interoperabile reports and data-curation tasks. For instance, The provenance related to specific data can be stored in W3C-PROV XML format into strucutred file formats (NetCDF) as well as istitutional and general-purpose citable data-repositories.</li>\n",
    "</ul>\n",
    "\n",
    "## Demonstration outline\n",
    "\n",
    "### 1 - Workflow specification and execution\n",
    "\n",
    "<ol>\n",
    "  <li>Define the <i><b>Classes</b></i> of the <i><b>Workflow Components</b></i></li>\n",
    "  <li>Construct the <i><b>Workflow</b></i> application</li>\n",
    "  <li>Prepare the Input</li>\n",
    "  <li>Visualise and run the workflow without provenance</li>\n",
    "</ol>\n",
    "\n",
    "### 2 - Provenance Types, Configuration and Contextualisation\n",
    "\n",
    "<ol>\n",
    "  <li>Define the <i><b>Provenance Types</b></i> to be used within the workflow</li>\n",
    "  <li><i><b>Configure</b></i> the workfow for provenance tracking</li>\n",
    "  <li>Visualise and run workfow with provenance activatied</li>\n",
    "  <li>Export and embed provenance within NetCDF results</li>\n",
    "  <li>Explore the resulting provenance with interactive and static visualsations</li>\n",
    "</ol>\n",
    "\n",
    "### 3 - Data-reuse traceability. \n",
    "<ol>\n",
    "  <li>Change the input and demostrate consistency of provenance for data-ruse across multiple workflow executions</li>\n",
    "  <li>Discuss more complex use cases and configuration options</li>\n",
    "</ol>\n",
    "\n",
    "### 4 - Informal Evaluation\n",
    "\n",
    "SWOT form:\n",
    "\n",
    "https://docs.google.com/presentation/d/10xlRYytR7NB9iC19T29BD-rW77ZAtnjtlukMJDP_MIs/edit?usp=sharing\n",
    "\n",
    "\n",
    "## 1 - Workflow specification and execution\n",
    "\n",
    "\n",
    "<ul>\n",
    "<li>The dispel4py framework is utilised for the workflows</li>\n",
    "<li>Xarray for in-memory management of netcdf data.</li>\n",
    "<li>Matplotlib for visualisation.</li>\n",
    "<li>W3C for provenance representation.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xarray\n",
    "import netCDF4\n",
    "import json\n",
    "\n",
    "from dispel4py.workflow_graph import WorkflowGraph \n",
    "from dispel4py.provenance import *\n",
    "\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import random\n",
    "\n",
    "from dispel4py.base import create_iterative_chain, ConsumerPE, IterativePE, SimpleFunctionPE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Simple Workflow, xarray in xarray out. \n",
    "The generic processing elements are defined below. the <i>GenericPE</i> bellongs to the dispel4py framework. It allows data-objects to be passed as inputs and outputs. The <i>Components</i> are linked and visualised via the workflow_graph module.\n",
    "\n",
    "### 1.1 The Four Workflow Components:\n",
    "\n",
    "<ol>\n",
    "<li>- Read, xarray is read into memory.</li>\n",
    "<li>- ANALYSIS, xarray is processed/passed to output (dummy, no real changes in the example)</li>\n",
    "<li>- Write, xarray is visualised.</li>\n",
    "<li>- Combine, two xarray are combined into one ds.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Read(GenericPE):\n",
    "    \n",
    "    def __init__(self):\n",
    "        GenericPE.__init__(self)\n",
    "        self._add_input('input')\n",
    "        self._add_output('xarray')\n",
    "        self.count=0\n",
    "\n",
    "    def _process(self,inputs):\n",
    "        self.log('Read_Process')\n",
    "     \n",
    "        self.log(inputs)\n",
    "        \n",
    "        inputLocation = inputs['input'][0]\n",
    "\n",
    "        ds = xarray.open_dataset( inputLocation )\n",
    "        \n",
    "        self.write( 'xarray' , (ds, self.count) , location=inputLocation )\n",
    "        self.count+=1\n",
    "            \n",
    "class Write(GenericPE):\n",
    "    \n",
    "    def __init__(self):\n",
    "        GenericPE.__init__(self)\n",
    "        self._add_input('input')\n",
    "        self._add_output('location')\n",
    "        self.count=0\n",
    "         \n",
    "         \n",
    "        \n",
    "    def _process(self,inputs):\n",
    "        self.log('Write_Function')\n",
    "         \n",
    "        outputLocation = \"data/new_\"+str(self.count)+\".nc\"\n",
    "        self.count+=1\n",
    "        inputs['input'][0].to_netcdf( outputLocation )\n",
    "         \n",
    "        self.write('location', outputLocation,location=outputLocation)\n",
    "        \n",
    "        \n",
    "        \n",
    "class Analysis(GenericPE):\n",
    "        \n",
    "    def __init__(self):\n",
    "        GenericPE.__init__(self)\n",
    "        self._add_input('input')\n",
    "        self._add_output('output')\n",
    "         \n",
    "        \n",
    "    def _process(self,inputs):\n",
    "        self.log('Analysis_process')\n",
    "         \n",
    "        nc = inputs['input'][0]\n",
    "        \n",
    "        #Apply some np mathematics on nc\n",
    "        \n",
    "        \n",
    "        #Write to the output channel\n",
    "        self.write('output', (nc,inputs['input'][1]),metadata={'myterm': 10,'prov:type':'clipc:Pre-processed','index':inputs['input'][1]},message=\"\")\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "class Combine(GenericPE):\n",
    "     \n",
    "    def __init__(self):\n",
    "        GenericPE.__init__(self)\n",
    "        self._add_input('combine1',grouping=[1])\n",
    "        self._add_input('combine2',grouping=[1])\n",
    "        self._add_output('combo')\n",
    "        self.count=0\n",
    "        self.data1=[]\n",
    "        self.data2=[]\n",
    "        self.nc1 = None\n",
    "        self.nc2 = None\n",
    "        self.out = None\n",
    "        \n",
    "    def _process(self,inputs):\n",
    "        self.log('Combine_process')\n",
    "        #self.log(inputs.keys())\n",
    "        \n",
    "        if 'combine1' in inputs.keys():\n",
    "            self.data1.append(inputs['combine1'][0])\n",
    "            \n",
    "        \n",
    "        if 'combine2' in inputs.keys():\n",
    "            self.data2.append(inputs['combine2'][0])\n",
    "            \n",
    "        \n",
    "         \n",
    "        if (len(self.data1)>0 and len(self.data2)>0):\n",
    "            \n",
    "            nc1 =  self.data1.pop(0)\n",
    "            nc2 =  self.data2.pop(0)\n",
    "            \n",
    "            nc=nc1\n",
    "            # numpy arithmetic for DataArrays...\n",
    "             \n",
    "            self.log(nc2[self.parameters['var']][0][0][0])\n",
    "            \n",
    "            \n",
    "            \n",
    "            for k,v in nc2.attrs.items():\n",
    "                if k in nc.attrs.keys():\n",
    "                    nc.attrs[k] = v\n",
    "                else:\n",
    "                    nc.attrs[k] = v\n",
    "            \n",
    "           \n",
    "            \n",
    "            self.write('combo', (nc,self.count))\n",
    "            self.count+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Construct the Workflow application\n",
    "\n",
    "Instantiates the Components and combines them in a workflow graph which gets eventually visualised.\n",
    "\n",
    "Siz Instances are created from the above PEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Initialise the graph\n",
    "def createWorkflowGraph():\n",
    "    readX  = Read()\n",
    "    readX.name = 'COLLECTOR1'\n",
    "    readY  = Read()\n",
    "    readY.name = 'COLLECTOR2'\n",
    "    \n",
    "    analyse   = Analysis()\n",
    "    analyse.name    = 'ANALYSIS'\n",
    "    analyse.parameters = { 'var':'SWE','filter': 10 }\n",
    "\n",
    "    analyse2   = Analysis()\n",
    "    analyse2.name    = 'ANALYSIS'\n",
    "    analyse2.parameters = { 'var':'SWE','filter': 1 }\n",
    "    \n",
    "    wf3     = Combine()\n",
    "    wf3.name    = 'COMBINE'\n",
    "    wf3.parameters = { 'var':'SWE' }\n",
    "    \n",
    "    writeX = Write()\n",
    "    writeX.name = 'STORE'\n",
    "    \n",
    "    \n",
    "    graph = WorkflowGraph()    \n",
    "    \n",
    "    graph.connect(readX ,'xarray'   , analyse      ,'input')\n",
    "    graph.connect(readY ,'xarray'   , analyse2     ,'input')\n",
    "    \n",
    "    graph.connect( analyse  ,'output'   , wf3     ,'combine1')\n",
    "    graph.connect( analyse2 ,'output'   , wf3     ,'combine2')\n",
    "    \n",
    "    graph.connect(wf3    ,'combo'   , writeX , 'input')\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "\n",
    "graph = createWorkflowGraph()\n",
    "\n",
    "\n",
    "from dispel4py.visualisation import display\n",
    "display(graph)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Specify the Input\n",
    "\n",
    "A simple json representation is used to define initial input data for each named Component of the workflow.\n",
    "Every component can recieve a list of inputs. These will be streamed serially or in parallel, depending from the execution mode.\n",
    "\n",
    "#### Data Collected from external URLs\n",
    "\n",
    "Actual data collected from a thredds repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name=\"new\"\n",
    "\n",
    "input_data = {     \n",
    "                'COLLECTOR1': [ { 'input' : [ 'http://opendap.knmi.nl/knmi/thredds/dodsC/CLIPC/cmcc/SWE/SWE_ophidia-0-10-1_CMCC_GlobSnow-SWE-L3B_monClim_19791001-20080701_1979-2008.nc']},\n",
    "                               { 'input' : [ 'http://opendap.knmi.nl/knmi/thredds/dodsC/CLIPC/cmcc/SWE/SWE_ophidia-0-10-1_CMCC_GlobSnow-SWE-L3B_monClim_19791001-20080701_1979-2008.nc']}],\n",
    "                'COLLECTOR2': [ { 'input' : [ 'http://opendap.knmi.nl/knmi/thredds/dodsC/CLIPC/cmcc/SWE/SWE_ophidia-0-10-1_CMCC_GlobSnow-SWE-L3B_monClim_19791001-20080701_1979-2008.nc']},\n",
    "                               { 'input' : [ 'http://opendap.knmi.nl/knmi/thredds/dodsC/CLIPC/cmcc/SWE/SWE_ophidia-0-10-1_CMCC_GlobSnow-SWE-L3B_monClim_19791001-20080701_1979-2008.nc']}]\n",
    "                \n",
    "             }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternatively!!\n",
    "\n",
    "Data Collected from local archive (for data re-use demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = {     \n",
    "                'COLLECTOR1': [ { 'input': [ 'data/local_1.nc']},{ 'input': [ 'data/local_1.nc']}],\n",
    "                'COLLECTOR2': [ { 'input': [ 'data/local_0.nc']},{ 'input': [ 'data/local_0.nc']}]\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to run the workflow\n",
    "\n",
    "Applies the single process mode for demonstration purposes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def runExampleWorkflow(graph):\n",
    "                                                     \n",
    "    print input_data                   \n",
    "\n",
    "    #Launch in simple process\n",
    "    result = simple_process.process_and_return(graph, input_data)\n",
    "    print \"\\n RESULT: \"+str(result)\n",
    "\n",
    "#runExampleWorkflow(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Provenance Types, Configuration and Contextualisation\n",
    "\n",
    "### 2.1 Define a Provenance Type\n",
    "\n",
    "Once the Provenance types have been defined, these are used to configure, or configure a workflow execution to comply with the desired provenance collection requirements.  Below we illustrate the framework method and the details of this approach.\n",
    "\n",
    "<ul>\n",
    "\n",
    "<li><b><i>configure_prov_run</i></b> With this method, the users of the workflow can configure their run for provenance by indicating which types to apply to each component. Users can also chose where to store the metadata, locally to the file system or to a remote service. These operations can be performed in bulks, with different impacts on the overall overhead and on the experienced rapidity of the access of the lineage information. Finally, also general information about the attribution of the run, such as <i>username, run_id, description, workflow_name, workflow_id</i> are captured and included within the provenance traces.\n",
    "</li>\n",
    "<li><b><i>apply_derivation_rule (Advanced)</i></b>\n",
    "This method is invoked by each iteration when a decision has to be made on the required lineage pattern. The framework automatically passes information whether the invocation has produced any output or not. The method, according to predefined rules, provides indications on either discarding the current input data or to include it into the <i>StateCollection</i> automatically, capturing its contribution to the next invocation. \n",
    "In our implementation, basic provenance types such are available and can be used accordingly the specific needs.\n",
    "</li>\n",
    "<li><b><i>Selectiviy-Rules (Advanced)</i></b>\n",
    "Users can tune the scale of the records produced by indicating in the above method a set of <i>Selectiviy-Rules</i> for every component. This functionality allows users to specify rules to control the data-driven production of the provenance declaratively. The approach takes advantage of the contextualisation applied by the provenance types, which extract domain and experimental metadata, and evaluates their value against simple <i>Selectiviy-Rules</i>\n",
    "</li>\n",
    "</ul>\n",
    "\n",
    "Type for contextual metadata in netcdf:\n",
    "\n",
    "<b>netcdfProvType</b>: applicable to components that deal with a data formats containing array-oriented scientific data, including multiple variables and attributes associated with standard vocabularies and uid schemas.\n",
    "\n",
    "Type capturing provenance patterns:\n",
    "\n",
    "<b>Nby1Flow:</b> manges lineage of a component whose output depends on the data received on all its input ports in\n",
    "lock-step; e.g. combined analysis of multiple variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class netcdfProvType(ProvenanceType):\n",
    "    def __init__(self):\n",
    "        ProvenanceType.__init__(self)\n",
    "        self.addNamespacePrefix(\"clipc\",\"http://clipc.eu/ns/#\")\n",
    "    \n",
    "    def extractDataSourceId(self,data, input_port):\n",
    "        #Extract here the id from the data (type specific):\n",
    "\n",
    "        self.log('ANDREJ.extractExternalInputDataId')\n",
    "         \n",
    "        \n",
    "        try:\n",
    "             \n",
    "            ds = xarray.open_dataset(data[0])\n",
    "            id = ds.attrs['id']\n",
    "             \n",
    "             \n",
    "            \n",
    "            \n",
    "        except Exception, err:\n",
    "            id = str(uuid.uuid1())\n",
    "             \n",
    "       \n",
    "        return id\n",
    "     \n",
    "    \n",
    "    def makeUniqueId(self, data, output_port):      \n",
    "        \n",
    "        #self.log('ANDREJ.makeUniqueId')\n",
    "        \n",
    "        #produce the id\n",
    "        id=str(uuid.uuid1())\n",
    "            \n",
    "        ''' if nc data '''\n",
    "        if data!=None:\n",
    "            xa = data[0]\n",
    "        \n",
    "            ''' unique as defined by the community standard '''\n",
    "            xa.attrs['id'] = id\n",
    "        \n",
    "        #Return\n",
    "        return id \n",
    "    \n",
    "\n",
    "    \n",
    "    ''' extracts xarray metadata '''\n",
    "    def extractItemMetadata(self, data, output_port):\n",
    "         \n",
    "        \n",
    "        try:            \n",
    "            nc_meta = OrderedDict()\n",
    "            \n",
    "            ''' cycle throug all attributes, dimensions and variables '''\n",
    "            xa = data[0]\n",
    "            # dataset meta\n",
    "            nc_meta['Dimensions'] = str( dict(xa.dims)) \n",
    "            nc_meta['Type'] = str(type(xa))\n",
    "            \n",
    "             \n",
    "            for n , i in xa.data_vars.items():\n",
    "                for k , v in i.attrs.items():\n",
    "                    nc_meta['clipc:'+n+\"_\"+str(k).replace(\".\",\"_\")] = str(v)[0:25]\n",
    "            \n",
    "           \n",
    "            metadata = [nc_meta]\n",
    "            \n",
    "            return metadata\n",
    "                             \n",
    "        except Exception, err:\n",
    "            self.log(\"Applying default metadata extraction\"+str(traceback.format_exc()))\n",
    "            self.error=self.error+\"Applying default metadata extraction:\"+str(traceback.format_exc())\n",
    "            return super(netcdfProvType, self).extractItemMetadata(data,output_port);\n",
    "        \n",
    "        \n",
    "        \n",
    "class Nby1Flow(ProvenanceType):\n",
    "    def __init__(self):\n",
    "        ProvenanceType.__init__(self)\n",
    "        self.ports_lookups={}\n",
    "        \n",
    "\n",
    "    def apply_derivation_rule(self,event,voidInvocation,oport=None,data=None,iport=None,metadata=None):\n",
    "    \n",
    "         \n",
    "# reacts on specific events (write, end_invocation_event). \n",
    "#The latter is be characterised by having returned data (voidInvocation)\n",
    "        if (event=='write'):\n",
    "            dep=[]\n",
    "            for x in self.inputconnections:\n",
    "                if x!=iport and x!='_d4py_feedback':\n",
    "                    vv=self.ports_lookups[x].pop(0)\n",
    "                    dep.append(vv)\n",
    "                    #self.log(\"LOOKUP: \"+str(vv))\n",
    "                self.setStateDerivations(dep)\n",
    "                \n",
    "\n",
    "        if (event=='end_invocation_event' and voidInvocation==True):\n",
    "                \n",
    "            if data!=None:\n",
    "                #self.ports_lookups['iport'].append(vv)\n",
    "                vv=str(abs(make_hash(tuple(iport+str(self.iterationIndex)))))\n",
    "                if not (iport in self.ports_lookups):\n",
    "                    self.ports_lookups[iport]=[]\n",
    "\n",
    "                self.ports_lookups[iport].append(vv)\n",
    "                #self.log(self.ports_lookups)\n",
    "                #self.ignorePastFlow()\n",
    "                self.update_prov_state(vv,None,metadata={\"LOOKUP\":str(vv)})\n",
    "                self.discardInFlow()\n",
    "\n",
    "\n",
    "        if (event=='end_invocation_event' and voidInvocation==False):\n",
    "                 self.discardInFlow()\n",
    "                 self.discardState()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'configuration' describing the provenance setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "selrule = {\"ANALYSIS\": { \n",
    "                         \"rules\":{ \n",
    "                                 \"myterm\": {\n",
    "                                            \"$lt\": 15 }\n",
    "                                }\n",
    "                        }\n",
    "           }\n",
    "\n",
    "prov_config =  {\n",
    "                    'provone:User': \"aspinuso\", \n",
    "                    's-prov:description' : \"provdemo combo wet days\",\n",
    "                    's-prov:workflowName': \"demo_ecmwf\",\n",
    "                    's-prov:workflowType': \"clipc:combine\",\n",
    "                    's-prov:workflowId'  : \"workflow process\",\n",
    "                    's-prov:save-mode'   : 'service'         ,\n",
    "                    # defines the Provenance Types and Provenance Clusters for the Workflow Components\n",
    "                    's-prov:componentsType' : \n",
    "                                       {'ANALYSIS': {'s-prov:type':(netcdfProvType,),\n",
    "                                                     's-prov:prov-cluster':'clipc:Combiner'},\n",
    "                                        'COMBINE':  {'s-prov:type':(netcdfProvType, Nby1Flow,),\n",
    "                                                     's-prov:prov-cluster':'clipc:Combiner'},\n",
    "                                        'COLLECTOR1':{'s-prov:prov-cluster':'clipc:DataHandler,',\n",
    "                                                     's-prov:type':(netcdfProvType,)},\n",
    "                                        'COLLECTOR2':{'s-prov:prov-cluster':'clipc:DataHandler,',\n",
    "                                                     's-prov:type':(netcdfProvType,)},\n",
    "                                        'STORE':    {'s-prov:prov-cluster':'clipc:DataHandler'}\n",
    "                                        },\n",
    "                    's-prov:sel-rules': selrule\n",
    "                } \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The REPOS_URL is the target provenence repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Store via service\n",
    "ProvenanceType.REPOS_URL='http://ec2-18-197-219-251.eu-central-1.compute.amazonaws.com/workflowexecutions/insert'\n",
    "ProvenanceType.PROV_EXPORT_URL='http://ec2-18-197-219-251.eu-central-1.compute.amazonaws.com/data/'\n",
    "\n",
    "\n",
    "#Store to local path\n",
    "ProvenanceType.PROV_PATH='./prov-files/'\n",
    "\n",
    "#Size of the provenance bulk before sent to storage or sensor\n",
    "ProvenanceType.BULK_SIZE=1\n",
    "\n",
    "#ProvenancePE.REPOS_URL='http://climate4impact.eu/prov/workflow/insert'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createGraphWithProv():\n",
    "    \n",
    "    graph=createWorkflowGraph()\n",
    "    #Location of the remote repository for runtime updates of the lineage traces. Shared among ProvenanceRecorder subtypes\n",
    "\n",
    "    # Ranomdly generated unique identifier for the current run\n",
    "    rid='JUP_COMBINE_'+getUniqueId()\n",
    "\n",
    "    \n",
    "    # Finally, provenance enhanced graph is prepared:\n",
    "    \n",
    "\n",
    "     \n",
    "    #Initialise provenance storage to service:\n",
    "    configure_prov_run(graph, \n",
    "                     provImpClass=(ProvenanceType,),\n",
    "                     username=prov_config['provone:User'],\n",
    "                     runId=rid,\n",
    "                     description=prov_config['s-prov:description'],\n",
    "                     workflowName=prov_config['s-prov:workflowName'],\n",
    "                     workflowType=prov_config['s-prov:workflowType'],\n",
    "                     workflowId=prov_config['s-prov:workflowId'],\n",
    "                     save_mode=prov_config['s-prov:save-mode'],\n",
    "                     componentsType=prov_config['s-prov:componentsType'],\n",
    "                     sel_rules=prov_config['s-prov:sel-rules']\n",
    "                      \n",
    "                    )\n",
    "    \n",
    "    return graph\n",
    "\n",
    "\n",
    "graph=createGraphWithProv()\n",
    "\n",
    "display(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution with provenance\n",
    "The following script executes the workflow in single-process mode\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "runExampleWorkflow(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Visualise Provenance Trace\n",
    "\n",
    "Read the if of the output to locate the provenance trace on the remote service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2.3.1 Visualise in S-ProvFlow\n",
    "\n",
    "The following link opens a local installation of the S-ProvFlow System GUI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "http://localhost:8180/provenance-explorer/html/view.jsp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Extract from S-ProvFlow API \n",
    "\n",
    "The following scripts extracts the provenance of the last file produced in standard PROV-XML, and embed it into the  file itself\n",
    "\n",
    "Extraction from the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#''' read id of output to locate prov '''\n",
    "\n",
    "finalFile = 'data/new_'+str(len(input_data['COLLECTOR1'])-1)+'.nc'\n",
    "from shutil import copyfile\n",
    "\n",
    "#copyfile(finalFile, 'data/newA.nc')\n",
    "ds = xarray.open_dataset(finalFile)\n",
    "dataid = ds.attrs['id']     #\"orfeus-as-73355-c381c282-d422-11e6-ac42-f45c89acf865\"\n",
    " \n",
    "print(\"Extract Trace for dataid: \"+dataid)\n",
    "expurl = urlparse(ProvenanceType.PROV_EXPORT_URL)\n",
    "connection = httplib.HTTPConnection(expurl.netloc)\n",
    "url=\"http://\"+expurl.netloc+expurl.path+dataid+\"/export?level=100&format=xml\"\n",
    "print(url)\n",
    "connection.request(\n",
    "                \"GET\", url)\n",
    "response = connection.getresponse()\n",
    "print(\"progress: \" + str((response.status, response.reason)))\n",
    "prov1 = response.read()\n",
    "print('PROV TO EMBED:')\n",
    "print str(prov1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed the PROV document in the NetCDF file within the 'prov_xml'of the 'provenance' property and print the file's properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ds. create variable save to file\n",
    "\n",
    "ds.load()\n",
    "ds['provenance'] = xarray.DataArray(\"\")\n",
    "\n",
    "ds['provenance'].attrs['prov_xml']=str(prov1)\n",
    "\n",
    "ds.to_netcdf(str(finalFile+\"_PROV\"))\n",
    "ds = xarray.open_dataset(str(finalFile+\"_PROV\"))\n",
    "print ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the PROV document in its standard graph view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import prov\n",
    "import io\n",
    "import StringIO\n",
    "from prov.model import ProvDocument, ProvBundle, ProvException, first, Literal\n",
    "from prov.dot import prov_to_dot\n",
    "\n",
    "def provTo(xml,output_f):\n",
    "     \n",
    "    xml_doc = StringIO.StringIO()\n",
    "    xml_doc.write(str(xml))\n",
    "    xml_doc.seek(0, 0)\n",
    "    #print xml_doc\n",
    "    doc=ProvDocument.deserialize(xml_doc,format=\"xml\")\n",
    "    dot = prov_to_dot(doc)\n",
    "    return dot.create(format=output_f)\n",
    "\n",
    "\n",
    "png_content=provTo(prov1,\"png\")\n",
    "\n",
    "with open(\"PROV.png\",\"w+\") as text_file:\n",
    "    text_file.write(str(png_content))\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(\"PROV.png\")\n",
    "\n",
    "    \n",
    "    \n",
    "# visualse NetCDF provenance in PNG   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
