{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Correlation analysis example with Active-Provenance in dispel4py:\n",
    "\n",
    "\n",
    "\n",
    "### Sample Corss-Correlation Workflow: Description and Components\n",
    "<br/>\n",
    "\n",
    "The workflow performs and visualises the correlation matrix between a configurable number of sources and the resulting max cliques given a minimum correlation threshold. It can run over multipe iterations with parametrisable sampling-rate and length of the variable's batches to correlate.\n",
    "\n",
    "The workflow specification and especially its parametrisation are inspired by the following research paper:\n",
    "https://www.cs.ubc.ca/~hoos/Publ/RosEtAl07.pdf\n",
    "\n",
    "#### Components\n",
    "<br/>\n",
    "\n",
    "<li>1 - Class Start: Root node of the graph. It sends initial configruation parameters (Number of total number produced) </li>\n",
    "<li>2 - Class Source: Produces random number from 0,100 at a specified sampling-rate</li>\n",
    "<li>3 - Class CorrCoef: Calculates the Pearson's correlation coefficient of a specified amount of samples (batch) coming from two sourcs</li>\n",
    "<li>4 - Class CorrMatrix: Produces and visualises the cross correlation matrix for all Sources for each batch</li>\n",
    "<li>5 - Class MaxClique: Transfor the correlation matrix into a graph according to a correlation minimum threshold and computes the graphâ€™s max clique</li>\n",
    "\n",
    "The script below defines the components and declares the workflow. Its execution will show a visual representation of the abstract workfkow grap.\n",
    "\n",
    "\n",
    "### Function update_prov_state \n",
    "Adds an object and its metadata to the PEs state. This can be referenced from the user during write operations, increasing the lineage precision in stateful components.\n",
    "\n",
    "The accepted parameters are the following:\n",
    "\n",
    "#### Unnamed parameters:\n",
    "<li> 1 - <i>name</i>: for references to the provenance StateCollection, a name of the object is required. Using the same name will overwrite the reference</li>\n",
    "<li> 2 - <i>data</i>: data-object to be stored in the provenance state</li>\n",
    "\n",
    "#### Named Parameters:\n",
    "<li> 2 - <i>location</i>: url or path indicating the location of the data file, if any has been produced</li>\n",
    "<li> 3 - <i>metadata</i>: dictionary of key,values pairs od user-defined metadata associated to the object.</li>\n",
    "<li> 4 - <i>ignore_inputs</i>: If <b>True</b> the dependencies which are currently standing are ignored. Default: <b>True</b></li>\n",
    "<li> 5 - <i>dep</i>: a list of look-up terms to retrieve references to <i>DataElements</i> stored by any invocation into the StateCollection. This parameter automatically establishes new <i>wasDerivedFrom</i> relationships</li>\n",
    "<li> 6 - <i>control</i>: these are actionable instructions like s-prov:skip and s-prov:immediateAccess. These messages are used respectively to selectively discard the production of traces for specific data-stream elements or to trigger their materialisation into a file, to be transferred towards an external target resource.</li>\n",
    "<li> 7 - <i>messages</i>: developers may identify situations worth to be communicated, like errors or warning, by producing human readable messages.</li>\n",
    "<li> 8 - <i>format</i>: typically contains the mime-type of the data.</li>\n",
    "\n",
    "\n",
    "The same contract is availabe for the dispel4py native function <i>write</i>. The difference is that the data-object, instead of being stored, it will be passed to the other components of the workflow through the output port indicated by <i>name</i>.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from dispel4py.workflow_graph import WorkflowGraph \n",
    "from dispel4py.provenance import *\n",
    "from dispel4py.seismo.seismo import *\n",
    "from dispel4py.new.processor  import *\n",
    "import time\n",
    "import random\n",
    "import numpy\n",
    "import traceback \n",
    "from dispel4py.base import create_iterative_chain, GenericPE, ConsumerPE, IterativePE, SimpleFunctionPE\n",
    "from dispel4py.new.simple_process import process_and_return\n",
    "from dispel4py.visualisation import display\n",
    "\n",
    "import IPython\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.stats import pearsonr \n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "\n",
    "class Start(GenericPE):\n",
    "\n",
    "    def __init__(self):\n",
    "        GenericPE.__init__(self)\n",
    "        self._add_input('iterations')\n",
    "        self._add_output('output')\n",
    "        #self.prov_cluster=\"myne\"\n",
    "    \n",
    "    def _process(self,inputs):\n",
    "        \n",
    "        if 'iterations' in inputs:\n",
    "            inp=inputs['iterations']\n",
    "             \n",
    "            self.write('output',inp,metadata={'iterations':inp})\n",
    "            \n",
    "        #Uncomment this line to associate this PE to the mycluster provenance-cluster \n",
    "        #self.prov_cluster ='mycluster'\n",
    "\n",
    "class Source(GenericPE):\n",
    "\n",
    "    def __init__(self,sr,index,batchsize):\n",
    "        GenericPE.__init__(self)\n",
    "        self._add_input('iterations')\n",
    "        self._add_output('output')\n",
    "        self.sr=sr\n",
    "        self.var_index=index\n",
    "        self.batchsize=batchsize\n",
    "        #self.prov_cluster=\"myne\"\n",
    "         \n",
    "        self.parameters={'sampling_rate':sr,'batchsize':batchsize}\n",
    "        \n",
    "        #Uncomment this line to associate this PE to the mycluster provenance-cluster \n",
    "        #self.prov_cluster ='mycluster'\n",
    "        \n",
    "    \n",
    "    def _process(self,inputs):\n",
    "         \n",
    "        if 'iterations' in inputs:\n",
    "            iteration=inputs['iterations'][0]\n",
    "        \n",
    "        \n",
    "        batch=[]\n",
    "        it=1\n",
    "        #Streams out values at 1/self.sr sampling rate, until iteration>0\n",
    "        while (it<=iteration):\n",
    "            while (len(batch)<self.batchsize):\n",
    "                val=random.uniform(0,100)\n",
    "                time.sleep(1/self.sr)\n",
    "                batch.append(val)\n",
    "                \n",
    "            self.write('output',(it,self.var_index,batch),metadata={'var':self.var_index,'iteration':it,'batch':batch})\n",
    "            batch=[]\n",
    "            it+=1\n",
    "        \n",
    "\n",
    "class MaxClique(GenericPE):\n",
    "\n",
    "    def __init__(self,threshold):\n",
    "        GenericPE.__init__(self)\n",
    "        self._add_input('input')\n",
    "        self._add_output('graph')\n",
    "        self._add_output('clique')\n",
    "        self.threshold=threshold\n",
    "        #self.prov_cluster=\"myne\"\n",
    "         \n",
    "        self.parameters={'threshold':threshold}\n",
    "        \n",
    "                \n",
    "        #Uncomment this line to associate this PE to the mycluster provenance-cluster \n",
    "        #self.prov_cluster ='mycluster'\n",
    "        \n",
    "    \n",
    "    def _process(self,inputs):\n",
    "         \n",
    "        if 'input' in inputs:\n",
    "            matrix=inputs['input'][0]\n",
    "            iteration=inputs['input'][1]\n",
    "        \n",
    "        \n",
    "         \n",
    "       \n",
    "        \n",
    "        low_values_indices = matrix < self.threshold  # Where values are low\n",
    "        matrix[low_values_indices] = 0 \n",
    "        #plt.figure('graph_'+str(iteration))\n",
    "                \n",
    "         \n",
    "        \n",
    "        H = nx.from_numpy_matrix(matrix)\n",
    "        fig = plt.figure('graph_'+str(iteration))\n",
    "        text = \"Iteration \"+str(iteration)+\" \"+\"graph\"\n",
    "         \n",
    "        labels = {i : i for i in H.nodes()}\n",
    "        pos = nx.circular_layout(H)\n",
    "        nx.draw_circular(H)\n",
    "        nx.draw_networkx_labels(H, pos, labels, font_size=15)\n",
    "        fig.text(.1,.1,text)\n",
    "        self.write('graph',matrix,metadata={'graph':str(matrix),'batch':iteration})\n",
    "       \n",
    "         \n",
    "        \n",
    "       # labels = {i : i for i in H.nodes()}\n",
    "       # pos = nx.circular_layout(H)\n",
    "       # nx.draw_circular(H)\n",
    "       # nx.draw_networkx_labels(H, pos, labels, font_size=15)\n",
    "       \n",
    "        \n",
    "        cliques = list(nx.find_cliques(H))\n",
    "        \n",
    "        fign=0\n",
    "        maxcnumber=0\n",
    "        maxclist=[]\n",
    "        \n",
    "        for nodes in cliques:\n",
    "            if (len(nodes)>maxcnumber):\n",
    "                maxcnumber=len(nodes)\n",
    "                \n",
    "        for nodes in cliques:\n",
    "            if (len(nodes)==maxcnumber):\n",
    "                maxclist.append(nodes)\n",
    "\n",
    "        for nodes in maxclist:    \n",
    "            edges = combinations(nodes, 2)\n",
    "            C = nx.Graph()\n",
    "            C.add_nodes_from(nodes)\n",
    "            C.add_edges_from(edges)\n",
    "            fig = plt.figure('clique_'+str(iteration)+str(fign))\n",
    "            text = \"Iteration \"+str(iteration)+\" \"+\"clique \"+str(fign)\n",
    "            fign+=1\n",
    "            labels = {i : i for i in C.nodes()}\n",
    "            pos = nx.circular_layout(C)\n",
    "            nx.draw_circular(C)\n",
    "            nx.draw_networkx_labels(C, pos, labels, font_size=15)\n",
    "            fig.text(.1,.1,text)\n",
    "            self.write('clique',cliques,metadata={'clique':str(nodes),'iteration':iteration, 'order':maxcnumber}, location=\"file://cliques/\")\n",
    "    \n",
    "              \n",
    "        \n",
    "#        C = nx.make_max_clique_graph(H)\n",
    "#        plt.figure('clique_'+str(iteration))\n",
    "#        labels = {i : i for i in C.nodes()}\n",
    "##        pos = nx.circular_layout(C)\n",
    "#        nx.draw_circular(C)\n",
    "#        nx.draw_networkx_labels(C, pos, labels, font_size=15)\n",
    "         \n",
    "       \n",
    "        #Streams out values at 1/self.sr sampling rate, until iteration>0\n",
    "        \n",
    "        \n",
    "\n",
    "class CorrMatrix(GenericPE):\n",
    "\n",
    "    def __init__(self,variables_number):\n",
    "        GenericPE.__init__(self)\n",
    "        self._add_input('input',grouping=[0]) \n",
    "        self._add_output('output')\n",
    "        self.size=variables_number\n",
    "        self.parameters={'variables_number':variables_number}\n",
    "        self.data={}\n",
    "         \n",
    "        \n",
    "        #Uncomment this line to associate this PE to the mycluster provenance-cluster \n",
    "        #self.prov_cluster ='mycluster'self.prov_cluster='mycluster'\n",
    "            \n",
    "    def _process(self,inputs):\n",
    "        for x in inputs:\n",
    "            \n",
    "            if inputs[x][0] not in self.data:\n",
    "                #prepares the data to visualise the xcor matrix of a specific batch number.\n",
    "                self.data[inputs[x][0]]={}\n",
    "                self.data[inputs[x][0]]['matrix']=numpy.identity(self.size)\n",
    "                self.data[inputs[x][0]]['ro_count']=0\n",
    "            \n",
    "            if (inputs[x][1][0]>inputs[x][1][1]):\n",
    "                self.data[inputs[x][0]]['matrix'][inputs[x][1][0]-1,inputs[x][1][1]-1]=inputs[x][2]\n",
    "            if (inputs[x][1][0]<inputs[x][1][1]):\n",
    "                self.data[inputs[x][0]]['matrix'][inputs[x][1][1]-1,inputs[x][1][0]-1]=inputs[x][2]\n",
    "                \n",
    "            self.data[inputs[x][0]]['matrix'][inputs[x][1][0]-1,inputs[x][1][1]-1]=inputs[x][2]\n",
    "            self.data[inputs[x][0]]['ro_count']+=1\n",
    "            #self.log((inputs[x][0],self.data[inputs[x][0]]['ro_count']))\n",
    "            ##self.update_prov_state('iter_'+str(inputs[x][0]),None,metadata={'iter_'+str(inputs[x][0]):inputs[x][1]},dep=['iter_'+str(inputs[x][0])])\n",
    "            \n",
    "            if self.data[inputs[x][0]]['ro_count']==(self.size*(self.size-1))/2:\n",
    "                matrix=self.data[inputs[x][0]]['matrix']\n",
    "                \n",
    "                d = pd.DataFrame(data=matrix,\n",
    "                 columns=range(0,self.size),index=range(0,self.size))\n",
    "                \n",
    "                mask = numpy.zeros_like(d, dtype=numpy.bool)\n",
    "                mask[numpy.triu_indices_from(mask)] = True\n",
    "\n",
    "                # Set up the matplotlib figure\n",
    "                f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "                # Generate a custom diverging colormap\n",
    "                cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "                # Draw the heatmap with the mask and correct aspect ratio\n",
    "                sns.heatmap(d, mask=mask, cmap=cmap, vmax=1,\n",
    "                    square=True,\n",
    "                    linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n",
    "                \n",
    "                sns.plt.show()   \n",
    "                self.log('\\r\\n'+str(matrix))\n",
    "                self.write('output',(matrix,inputs[x][0]),metadata={'matrix':str(d),'iteration':str(inputs[x][0])})\n",
    "                ##dep=['iter_'+str(inputs[x][0])])\n",
    "\n",
    "\n",
    "            \n",
    "class CorrCoef(GenericPE):\n",
    "\n",
    "    def __init__(self):\n",
    "        GenericPE.__init__(self)\n",
    "        #self._add_input('input1',grouping=[0])\n",
    "        #self._add_input('input2',grouping=[0])\n",
    "        self._add_output('output')\n",
    "        self.data={}\n",
    "        \n",
    "        \n",
    "         \n",
    "        \n",
    "    def _process(self, inputs):\n",
    "        index=None\n",
    "        val=None\n",
    "              \n",
    "        for x in inputs:\n",
    "            if inputs[x][0] not in self.data:\n",
    "                self.data[inputs[x][0]]=[]\n",
    "                \n",
    "            for y in self.data[inputs[x][0]]:\n",
    "                #self.log([y,self.data])\n",
    "                ro=numpy.corrcoef(y[2],inputs[x][2])\n",
    "                #self.log(((inputs[x][0],(y[0],inputs[x][1]),ro[0][1])))\n",
    "                self.write('output',(inputs[x][0],(y[1],inputs[x][1]),ro[0][1]),metadata={'iteration':inputs[x][0],'vars':str(y[1])+\"_\"+str(inputs[x][1]),'rho':ro[0][1]},dep=['var_'+str(y[1])+\"_\"+str(y[0])])\n",
    "                #self.write('output',(inputs[x][0],(y[0],inputs[x][1]),ro[0][1]),metadata={'iteration':inputs[x][0],'vars':str(y[0])+\"_\"+str(inputs[x][1]),'ro':ro[0][1]})\n",
    "            \n",
    "            \n",
    "            \n",
    "            #appends var_index and value\n",
    "            self.data[inputs[x][0]].append(inputs[x])\n",
    "            #self.log(self.data[inputs[x][0]])\n",
    "            self.update_prov_state('var_'+str(inputs[x][1])+\"_\"+str(inputs[x][0]),None,metadata={'var_'+str(inputs[x][1]):inputs[x][2]}, ignore_inputs=True)\n",
    "            \n",
    "     \n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing workflow inputs and parameters\n",
    "\n",
    "<b>Declare workflow inputs:</b>\n",
    "Eeach <i>iteration</i> prduces a batch of samples of size <i>batch_size</i> at the specified <i>sampling_rate</i> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "variables_number=20\n",
    "sampling_rate=100\n",
    "batch_size=3\n",
    "iterations=3\n",
    "\n",
    "input_data = {\"Start\": [{\"iterations\": [iterations]}]}\n",
    "      \n",
    "# Instantiates the Workflow Components  \n",
    "# and generates the graph based on parameters\n",
    "\n",
    "def createWf():\n",
    "    graph = WorkflowGraph()\n",
    "    mat=CorrMatrix(variables_number)\n",
    "    mat.prov_cluster='record2'\n",
    "    mc = MaxClique(-0.5)\n",
    "    mc.prov_cluster='record0'\n",
    "    start=Start()\n",
    "    start.prov_cluster='record0'\n",
    "    sources={}\n",
    "     \n",
    "\n",
    "    \n",
    "    \n",
    "    cc=CorrCoef()\n",
    "    cc.prov_cluster='record1'\n",
    "    stock=['NASDAQ','MIBTEL','DOWJ']\n",
    "      \n",
    "    for i in range(1,variables_number+1):\n",
    "        sources[i] = Source(sampling_rate,i,batch_size)\n",
    "        sources[i].prov_cluster=stock[i%len(stock)-1]\n",
    "         \n",
    "        \n",
    "        #'+str(i%variables_number)\n",
    "        #+str(i%7)\n",
    "        sources[i].numprocesses=1\n",
    "        #sources[i].name=\"Source\"+str(i)\n",
    "\n",
    "    for h in range(1,variables_number+1):\n",
    "        cc._add_input('input'+'_'+str(h+1),grouping=[0])\n",
    "        graph.connect(start,'output',sources[h],'iterations')\n",
    "        graph.connect(sources[h],'output',cc,'input'+'_'+str(h+1))\n",
    "        \n",
    "    \n",
    "    graph.connect(cc,'output',mat,'input')\n",
    "    graph.connect(mat,'output',mc,'input')\n",
    "    \n",
    "        \n",
    "  \n",
    "    return graph\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution without provenance\n",
    "The followin instruction executes the workflow in single-process mode with no provenance\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#graph=createWf()\n",
    "#Visualise the graph\n",
    "\n",
    "#display(graph)\n",
    "\n",
    "print (\"Preparing for: \"+str(iterations)+\" projections\" )\n",
    "\n",
    "#Launch in simple process\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "#process_and_return(graph, input_data)\n",
    "elapsed_time = time.time() - start_time\n",
    "print (\"ELAPSED TIME: \"+str(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the workflow graph for provenance production, pre-analysis and storage\n",
    "\n",
    "This snippet will make sure that the workflow compoentns will be provenance-aware and the lineage information sent to the designated ProvenanceRecorders for in-workflow pre-analysis.\n",
    "\n",
    "The execution will show a new graph where it will be possible to validate the provenance-cluster, if any, and the correct association of ProvenanceRecorders and feedback connections.\n",
    "\n",
    "The graph will change according to the declaration of self.prov_cluster property of the processing elements and to the specification of different ProvenanceRecorders and feedback loops, as described below:\n",
    "\n",
    "### Function profile_prov_run \n",
    "Prepares the workflow with the required provenance mechanisms\n",
    "The accepted parameters are the following:\n",
    "\n",
    "#### Unnamed parameters:\n",
    "<li> 1 - <i>worfklow graph</i></li>\n",
    "<li> 2 - Class name implementing the default <i>ProvenanceRecorder</i></li>\n",
    "\n",
    "#### Named Parameters\n",
    "<li> 3 - <i>provImpClass</i>: Class name extending the default <i>ProvenancePE</i>. The current type of the workflow components (GenericPE) will be extended with the one indicated by the <i>provImpClass</i> type</li>\n",
    "<li> 4 - <i>username</i></li>\n",
    "<li> 5 - <i>runId</i></li>\n",
    "<li> 6 - <i>w3c_prov</i>: specifies if the PE will outupt lineage in PROV format (default=False)</li>\n",
    "<li> 7 - <i>workflowName</i></li>\n",
    "<li> 8 - <i>workflowId</i></li>\n",
    "<li> 9 - <i>clustersRecorders</i>: dictionary associating <i>provenance-clusters</i> with a specific \n",
    "<li> 9 - <i>componentsType</i>: dictionary associating <i>PEs</i> with a specific \n",
    "<i>ProvenanceType</i> (overrides the default <i>provImpClass</i>) </li>\n",
    "<li> 10 - <i>feedbackPEs</i>: list of PE names receiving and processing feedbacks from the <i>ProvenanceRecorder</i>. </li>\n",
    "<li> 11 - <i>save_mode</i>: specifies if provenance has to be sent to service, stored to file or sent to a sensor for processing and/or storage.</li>\n",
    "\n",
    "<br/>\n",
    "\n",
    "#### Provenance Types\n",
    "\n",
    "Below we define a Class <i>ProvenanceStock</i> that defines the provenance properties that we will assign to the <i>Source</i> PEs. The <i>extractItemMetadata</i> produces the dictionary (list of dictionaries) to be stored as contextual metadata for each element of the computation. \n",
    "\n",
    "A provenance type can be also used to create high level abstractions based on the data produced.\n",
    "\n",
    "For instance, the following type aggreates the outputs produced from a <i>Source</i> in a single trace every 4 samples, reducing the provenance output and introducing a high level abstraction on the actual computation. The <i>provon</i> attribute tells the system if the traces should be produced or not.\n",
    "\n",
    "These traces may be less precise, but are useful to produce summaries and to reduce the provenance overhead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class ProvenanceStock(ProvenancePE):\n",
    "    def __init__(self):\n",
    "        ProvenancePE.__init__(self)\n",
    "        self.streammeta=[]\n",
    "        self.count=1\n",
    "    \n",
    "    \n",
    "    \n",
    "        def makeUniqueId(self,**kwargs):\n",
    "\n",
    "            #produce the id\n",
    "            id=str(uuid.uuid1())\n",
    "\n",
    "            #Store here the id into the data (type specific):\n",
    "            if 'data' in kwargs:\n",
    "                data=kwargs['data']\n",
    "\n",
    "            #Return\n",
    "            return id\n",
    "\n",
    "\n",
    "        def extractExternalInputDataId(self,data):\n",
    "\n",
    "            #Extract here the id from the data (type specific):\n",
    "            id = str(uuid.uuid1(str(data)))\n",
    "\n",
    "            #Return\n",
    "            return id\n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    def extractItemMetadata(self,data,port):\n",
    "        try:\n",
    "            metadata=None\n",
    "            self.embed=True\n",
    "            self.streammeta.append({'val':str(data)})\n",
    "             \n",
    "            if (self.count%1==0):\n",
    "                \n",
    "                metadata=deepcopy(self.streammeta)\n",
    "                self.provon=True\n",
    "                self.streammeta=[]\n",
    "            else:\n",
    "                self.provon=False\n",
    "            \n",
    "            self.count+=1\n",
    "            return metadata\n",
    "                \n",
    "                 \n",
    "\n",
    "        except Exception, err:\n",
    "            self.log(\"Applying default metadata extraction:\"+str(traceback.format_exc()))\n",
    "            self.error=self.error+\"Extract Metadata error: \"+str(traceback.format_exc())\n",
    "            return super(ProvenanceStock, self).extractItemMetadata(data,port);\n",
    "            \n",
    "        \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selrule1 = {\"CorrCoef\": { \n",
    "                         \"rules\":{ \n",
    "                                 \"rho\": {\n",
    "                                            \"$gt\": 0 }\n",
    "                                }\n",
    "                        }\n",
    "           }\n",
    "\n",
    "selrule2 = {\"Start\": { \n",
    "                         \"rules\":{ \n",
    "                                 \"iterations\": {\n",
    "                                            \"$lt\": 0 }\n",
    "                                }\n",
    "                        }\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Store via recorders or sensors\n",
    "#ProvenanceRecorder.REPOS_URL='http://127.0.0.1:8080/j2ep-1.0/prov/workflow/insert'\n",
    "\n",
    "#Store via service\n",
    "ProvenancePE.REPOS_URL='http://127.0.0.1:8082/workflow/insert'\n",
    "\n",
    "#Export data lineage via service (REST GET Call on dataid resource)\n",
    "ProvenancePE.PROV_EXPORT_URL='http://127.0.0.1:8082/workflow/export/data/'\n",
    "\n",
    "#Store to local path\n",
    "ProvenancePE.PROV_PATH='./prov-files/'\n",
    "\n",
    "#Size of the provenance bulk before sent to storage or sensor\n",
    "ProvenancePE.BULK_SIZE=20\n",
    "\n",
    "#ProvenancePE.REPOS_URL='http://climate4impact.eu/prov/workflow/insert'\n",
    "\n",
    "def createGraphWithProv():\n",
    "    \n",
    "    graph=createWf()\n",
    "    #Location of the remote repository for runtime updates of the lineage traces. Shared among ProvenanceRecorder subtypes\n",
    "\n",
    "    # Ranomdly generated unique identifier for the current run\n",
    "    rid='CORR_LARGE_'+getUniqueId()\n",
    "\n",
    "    \n",
    "    # Finally, provenance enhanced graph is prepared:   \n",
    "   \n",
    "    #Initialise provenance storage end associate a Provenance type with specific components:\n",
    "    prepare_prov_run(graph,None, provImpClass=(ProvenancePE,),\n",
    "                     username='aspinuso',\n",
    "                     runId=rid,\n",
    "                     input=[{'name':'variables_number','url':variables_number},\n",
    "                            {'name':'sampling_rate','url':sampling_rate},\n",
    "                            {'name':'batch_size','url':batch_size},\n",
    "                            {'name':'iterations','url':iterations}],\n",
    "                     w3c_prov=False,\n",
    "                     description=\"provState\",\n",
    "                     workflowName=\"test_rdwd\",\n",
    "                     workflowId=\"!23123\",\n",
    "                     componentsType= {'MaxClique':{'s-prov:type':(IntermediateStatefulOut,),\n",
    "                                                   'state_dep_port':'graph',\n",
    "                                                   's-prov:prov-cluster':'knmi:stockAnalyser'},\n",
    "                                      'CorrMatrix':{'s-prov:type':(ASTGrouped,),\n",
    "                                                    's-prov:prov-cluster':'knmi:stockAnalyser'},\n",
    "                                      'CorrCoef':{'s-prov:type':(SingleInvocationFlow,),\n",
    "                                                    's-prov:prov-cluster':'knmi:Correlator'}},\n",
    "                                      \n",
    "                                       save_mode='service',\n",
    "                                       sel_rules=selrule2)\n",
    "    return graph\n",
    "\n",
    "\n",
    "graph=createGraphWithProv()\n",
    "\n",
    "display(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution with provenance\n",
    "The followin instruction executes the workflow in single-process mode\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Launch in simple process\n",
    "start_time = time.time()\n",
    "process_and_return(graph, input_data)\n",
    "elapsed_time = time.time() - start_time\n",
    "print (\"ELAPSED TIME: \"+str(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing ProvenanceRecorders or Sensors\n",
    "\n",
    "The Class below show a sample <i>ProvenanceRecorderToService</i> and a slightlty more advanced one that allows for feedback.\n",
    "\n",
    "### ProvenanceRecorderToService\n",
    "\n",
    "Recieves traces from the PEs and sends them out to an exteranal provenance store.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class ProvenanceRecorderToService(ProvenanceRecorder):\n",
    "\n",
    "    def __init__(self, name='ProvenanceRecorderToService', toW3C=False):\n",
    "        ProvenanceRecorder.__init__(self)\n",
    "        self.name = name\n",
    "        self.numprocesses=2\n",
    "        self.convertToW3C = toW3C\n",
    "         \n",
    "    def _postprocess(self):\n",
    "        self.connection.close()\n",
    "        \n",
    "    def _preprocess(self):\n",
    "        self.provurl = urlparse(ProvenanceRecorder.REPOS_URL)\n",
    "        self.connection = httplib.HTTPConnection(\n",
    "            self.provurl.netloc)\n",
    "        \n",
    "    def sendToService(self,prov):\n",
    "        params = urllib.urlencode({'prov': ujson.dumps(prov)})\n",
    "        headers = {\n",
    "                    \"Content-type\": \"application/x-www-form-urlencoded\",\n",
    "                    \"Accept\": \"application/json\"}\n",
    "        self.connection.request(\n",
    "                    \"POST\",\n",
    "                    self.provurl.path,\n",
    "                    params,\n",
    "                    headers)\n",
    "\n",
    "        response = self.connection.getresponse()\n",
    "        self.log(\"Postprocress: \" +\n",
    "                 str((response.status, response.reason, response)))\n",
    "        self.connection.close()\n",
    "        \n",
    "    def process(self, inputs):\n",
    "        try:\n",
    "             \n",
    "            for x in inputs:\n",
    "                \n",
    "                prov = inputs[x]\n",
    "                \n",
    "                \n",
    "                if \"_d4p\" in prov:\n",
    "                    prov = prov[\"_d4p\"]\n",
    "                elif \"provenance\" in prov:\n",
    "                    prov = prov[\"provenance\"]\n",
    "                    \n",
    "                #self.log(prov)\n",
    "                self.sendToService(prov)\n",
    "                \n",
    "                \n",
    "        except:\n",
    "            self.log(traceback.format_exc())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provenance Sensor with Feedback - MyProvenanceRecorderWithFeedback\n",
    "\n",
    "Recieves traces from the PEs and reads its content. Depending from the 'name' of the PE sending the lineage, feedbacks are prepared and sent back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyProvenanceRecorderWithFeedback(ProvenanceRecorder):\n",
    "\n",
    "    def __init__(self, toW3C=False):\n",
    "        ProvenanceRecorder.__init__(self)\n",
    "        self.convertToW3C = toW3C\n",
    "        self.bulk = []\n",
    "        self.timestamp = datetime.datetime.utcnow()\n",
    "\n",
    "    def _preprocess(self):\n",
    "        self.provurl = urlparse(ProvenanceRecorder.REPOS_URL)\n",
    "\n",
    "        self.connection = httplib.HTTPConnection(\n",
    "            self.provurl.netloc)\n",
    "\n",
    "    def postprocess(self):\n",
    "        self.connection.close()\n",
    "        \n",
    "    def _process(self, inputs):\n",
    "        prov = None\n",
    "        for x in inputs:\n",
    "            prov = inputs[x]\n",
    "        out = None\n",
    "        if isinstance(prov, list) and \"data\" in prov[0]:\n",
    "            prov = prov[0][\"data\"]\n",
    "\n",
    "        if self.convertToW3C:\n",
    "            out = toW3Cprov(prov)\n",
    "        else:\n",
    "            out = prov\n",
    "\n",
    "            \n",
    "            \n",
    "        self.write(self.porttopemap[prov['name']], \"FEEDBACK MESSAGGE FROM RECORDER\")\n",
    "\n",
    "        self.bulk.append(out)\n",
    "        params = urllib.urlencode({'prov': json.dumps(self.bulk)})\n",
    "        headers = {\n",
    "            \"Content-type\": \"application/x-www-form-urlencoded\",\n",
    "            \"Accept\": \"application/json\"}\n",
    "        self.connection.request(\n",
    "            \"POST\", self.provurl.path, params, headers)\n",
    "        response = self.connection.getresponse()\n",
    "        self.log(\"progress: \" + str((response.status, response.reason,\n",
    "                                         response, response.read())))\n",
    "        \n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active provenance with feedback - TBD\n",
    "Here we show how to implement a PE that can handle feedback from a <i>ProvenanceRecoder</i>. We redefine the worklow graph with the same structure as the provious one but with the new PE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Workflow with provenance clusters and recorders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def createGraphWithProv():\n",
    "    \n",
    "    graph=createWf()\n",
    "    #Location of the remote repository for runtime updates of the lineage traces. Shared among ProvenanceRecorder subtypes\n",
    "\n",
    "    # Ranomdly generated unique identifier for the current run\n",
    "    rid='JUP_REC_'+getUniqueId()\n",
    "\n",
    "    #clustersRecorders={'recorder0':ProvenanceRecorderToFileBulk,'recorder0':ProvenanceRecorderToFileBulk,'recorder1':ProvenanceRecorderToFileBulk,'recorder2':ProvenanceRecorderToFileBulk,'recorder3':ProvenanceRecorderToFileBulk,'recorder4':ProvenanceRecorderToFileBulk,'recorder5':ProvenanceRecorderToFileBulk}\n",
    "    \n",
    "    #Initialise provenance storage to sensors and service:\n",
    "    #profile_prov_run(graph,ProvenanceRecorderToService,provImpClass=(ProvenancePE,),username='aspinuso',runId=rid,w3c_prov=False,description=\"provState\",workflowName=\"test_rdwd\",workflowId=\"xx\",save_mode='sensor')\n",
    "    #clustersRecorders=clustersRecorders,\n",
    "    \n",
    "    return graph\n",
    "#.. and visualised..\n",
    "\n",
    "\n",
    "\n",
    "# Instantiates the Workflow Components\n",
    "#graph=createGraphWithProv()\n",
    "\n",
    "#.. and visualises..\n",
    "\n",
    "display(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution with provenance recorders\n",
    "The followin instruction executes the workflow in single-process mode\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#graph = createGraphWithProv()\n",
    "#Launch in simple process\n",
    "start_time = time.time()\n",
    "#process_and_return(graph, input_data)\n",
    "#elapsed_time = time.time() - start_time\n",
    "#print (\"ELAPSED TIME: \"+str(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction from repository of a full provenance trace for a single data-id \n",
    "\n",
    "The following instructions connect to the online S-PROV WEB-API and downloads the PROV trace for a single data-id\n",
    "The trace may be used, for instace, as provenance metadata to embed into the data-file (eg. NetCDF)\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataid=\"orfeus-as-49601-2fb4d617-a508-11e7-a128-f45c89acf865\"\n",
    "print(\"Extract Trace for dataid: \"+dataid)\n",
    "\n",
    "expurl = urlparse(ProvenancePE.PROV_EXPORT_URL)\n",
    "connection = httplib.HTTPConnection(expurl.netloc)\n",
    "print(expurl.netloc+expurl.path+dataid+\"?all=true\")\n",
    "connection.request(\n",
    "                \"GET\", expurl.path+dataid+\"?all=true\")\n",
    "response = connection.getresponse()\n",
    "print(\"progress: \" + str((response.status, response.reason)))\n",
    "prov = response.read()\n",
    "print('PROV TO EMBED: '+str(prov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
